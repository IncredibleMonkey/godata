{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!\n",
    "Hypothesis & cost function에 대한 이해 후 코딩 이해합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H(x) = Wx+b\n",
    "여기서 W는 행렬(?)로 표현 - - - - 확인필요 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 줄의 코드만 이해해보자.<br>\n",
    "\" W = tf.Variable(tf.random_normal([1]), name='weight')\"\n",
    "\n",
    "- Variable : 강의에 따르면 반적으로 칭하는 \"변수\"와는 조금 다르다고 한다.<br>\n",
    "학습과정에서 tensorflow가 값을 자동으로 변경해준다는 의미라고 한다.<br>\n",
    "정확하게 이해되지는 않았지만 차차 진행하면서 이해하기로 한다.<br><br>\n",
    "\n",
    "- random_normal 의 괄호 안에는 shape이 들어간다.<br>\n",
    "lab 01의 마지막 부분에서 매우 간단히 다뤘었다. <br>\n",
    "여기서 tf.random_normal([1])은 1차원 배열의 하나의 원소만을 정규분포 랜덤으로 추출한다는 의미로 이해했다.<br><br>\n",
    "- name은 강의에서 아직 설명이 없었지만 그냥 말 그대로 해당 Variable의 name을 지정한 것 같다. (확인필요)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train에 있던 각 원소들이 해당 식(Lenear Function)에 하나씩 대입되면서 일직선으로 표현됨을 상상할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 변수 설명<br>\n",
    "위에서 정의했던 hypothesis는 가설함수(예측을 시도하는 값)이고 y_train는 실제값이다. <br>\n",
    "이 두 값의 차이가 작을수록 성공한 hypothesis일 것이다.<br><br>\n",
    "\n",
    "* tf.square<br>\n",
    "두 변수에 속한 각각의 값의 차이(예측값 - 실제값)을 측정하기 위하여 제곱을 한다.<br> * 텐서플로우에서 제곱을 square(사각형)이라는 이름으로 \n",
    "제공하나보다.<br><br>\n",
    "\n",
    "* reduce_mean<br>\n",
    "reduce_mean은 각 값의 차이를 구한 것을 모두 더하여 n으로 나누어주는 함수이다.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientDescent(경사하강법)\n",
    " 이론에서는 어렵게 배웠지만 여기에서는 매직 알고리즘으로 생각하고 그냥 사용한다. <br>\n",
    " 좋은게 좋은거니까.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer에서 learning_rate는 임의로 조정한 값이다.<br>\n",
    "이론강의에서 알파값으로 표현되는 learn rate는 0.01을 기본으로하여 관찰을 통해 조절하기를 권하였다.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법 알고리즘의 그래프를 만들었다\n",
    "learning rate가 0.01로 GradeintDecent가 적용된 optimizer을 이용하여 cost가 최소화하는 \"Graph\"를 그렸다.<br>\n",
    "\n",
    "* 괄호 안의 cost는 앞의 코드에서 이미 정의하였다.<br>\n",
    "\n",
    "##### 경사하강법 알고리즘 진행방식 \n",
    "    1. 아무 지점에서의 w를 선택한다.\n",
    "    2. W와 b를 조금씩 바꾼다.\n",
    "    3. 경사도(미분)를 계산하여 최저점에 도달할 때까지 반복한다.\n",
    "    4. 이 공식을 반복적으로 실행해서 minimize된 cost값을 얻을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent 실행 및 결과값 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강의에서 반드시 실행시켜야 한다고 했는데 이유는 잘 모르겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36.429947 [-1.4631933] [-0.7644013]\n",
      "50 0.008349236 [0.8891398] [0.2342068]\n",
      "100 0.0063391663 [0.9075093] [0.21020326]\n",
      "150 0.0049831546 [0.9180124] [0.18637706]\n"
     ]
    }
   ],
   "source": [
    "for step in range(200):\n",
    "    sess.run(train)\n",
    "    if step % 50 ==0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 경사각을 따라 점차 Cost가 낮은 방향으로 이동하게 설계되었다.<br>\n",
    "\n",
    "따라서 위 코드와 같이 반복적으로 실행해줘야 한다.<br>\n",
    "\n",
    "위 코드에서는 총 200번 반복수행을 하였으며, 사용자에게 가시적으로 표현되는 것은 50번에 1번으로 설정했다.<br>\n",
    "또한, 표현되는 값들은 [수행횟수, cost값, 가중치(W), bias]이다.<br>\n",
    "\n",
    "##### print 단계에서 아주 흥미로운 발견을 했다!\n",
    " 각 step단계에서 sess이 값들을 일시적으로 기억하는 것 같다. <br>\n",
    "그래서 같은 step의 cost, W, b를 출력할 수 있었던 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder 이용하기\n",
    "lab 01에서 placeholder에 대한 내용을 설명하였다.<br>\n",
    "이번 강의에서는 x_train과 y_train을 먼저 설정하고 경사하강법을 위한 graph를 설계했었다.<br>\n",
    "\n",
    "placeholder을 이용하면 graph를 설계한 뒤 x_train과 y_train을 대입시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None])\n",
    "Y = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "hypothesis = X * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape을 None이라고 했는데 아무 값이나 들어올 수 있다는 뜻이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.58298e-07 [0.9994111] [0.00133861]\n",
      "500 2.3370399e-08 [0.99982274] [0.00040259]\n",
      "1000 2.1285065e-09 [0.9999464] [0.00012151]\n",
      "1500 1.9226813e-10 [0.999984] [3.656035e-05]\n",
      "2000 2.0942062e-11 [0.9999946] [1.1992121e-05]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],\n",
    "                                         feed_dict={X: [1, 2, 3, 4, 5], \n",
    "                                                     Y: [2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step % 500 == 0:\n",
    "        print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 설계된 경사하강법의 graph에 feed_dict에 X와 Y만 설정해주면 된다.<br><br>\n",
    "placehold하지 않은 것에 비하여 코드가 조금 길어졌는데 그 이유는 다음과 같다<br>\n",
    "\n",
    "placehold를 하지 않았을 때에는 train을 따라가며 cost, W, b 값들이 저장이 됐었다.<br>\n",
    "하지만 placehold를 하면 x_train과 y_train이 처음에 설정되지 않았기 때문에 하나씩 temp변수로 넣어서 출력되도록 해줘야 한다.\n",
    "\n",
    "** 위의 코드에서 cost_val, W_val, b_val, _라고 적은 부분이 있는데 _는 train에 대한 값을 따로 저장하지 않겠다는 의미이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w와 b가 적절하게 선택됐는지 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W :  [0.9999946]\n",
      "b :  [1.1992121e-05]\n"
     ]
    }
   ],
   "source": [
    "# W와 b값 확인\n",
    "print(\"W : \", sess.run(W))\n",
    "print(\"b : \", sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0000012]\n",
      "[2.9999957]\n",
      "[9.999958]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 : 입력한 X의 값과 비슷하게 결과값이 나올 수록 좋은 성능을 보이는거다!\n",
    "print(sess.run(hypothesis, feed_dict={X:[2]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[3]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[10]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
